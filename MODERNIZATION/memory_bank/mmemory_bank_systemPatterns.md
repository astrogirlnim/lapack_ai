# LAPACK AI System Patterns - The "How"

## Architecture Overview

### High-Level System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LAPACK AI Containerized Stack               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Container Orchestration Layer                             â”‚
â”‚  â”œâ”€â”€ Docker Compose Development Environment                â”‚
â”‚  â”œâ”€â”€ Kubernetes Production Deployment                      â”‚
â”‚  â””â”€â”€ GPU Resource Management & Passthrough                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Python API Layer (lapack-py)                              â”‚
â”‚  â”œâ”€â”€ pybind11 Bindings                                     â”‚
â”‚  â”œâ”€â”€ NumPy Integration                                     â”‚
â”‚  â””â”€â”€ Error Translation                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Performance Monitor Dashboard                             â”‚
â”‚  â”œâ”€â”€ Flask Web Interface                                   â”‚
â”‚  â”œâ”€â”€ Real-time Metrics Collection                         â”‚
â”‚  â””â”€â”€ GPU/CPU Performance Comparison                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Enhanced Error Handling Layer                             â”‚
â”‚  â”œâ”€â”€ INFO Code Translation                                 â”‚
â”‚  â”œâ”€â”€ Condition Number Analysis                            â”‚
â”‚  â””â”€â”€ Diagnostic Recommendations                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GPU Acceleration Layer                                    â”‚
â”‚  â”œâ”€â”€ OpenCL Dispatch Logic                                â”‚
â”‚  â”œâ”€â”€ DGESVDOCL (GPU SVD)                                  â”‚
â”‚  â”œâ”€â”€ DGEMM_ALPHA (AlphaTensor 4Ã—4)                       â”‚
â”‚  â””â”€â”€ CPU Fallback Mechanisms                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core LAPACK Layer (Unmodified)                           â”‚
â”‚  â”œâ”€â”€ Original Fortran 90 Routines                         â”‚
â”‚  â”œâ”€â”€ BLAS Dependencies                                     â”‚
â”‚  â””â”€â”€ Reference Implementations                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Containerized System Dependencies                        â”‚
â”‚  â”œâ”€â”€ OpenBLAS                                             â”‚
â”‚  â”œâ”€â”€ OpenCL Runtime                                       â”‚
â”‚  â”œâ”€â”€ Python 3.11 Runtime                                 â”‚
â”‚  â””â”€â”€ GPU Drivers (Host Passthrough)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Design Patterns

### Pattern 1: VARIANTS System Integration (AlphaTensor) âœ…
**Principle**: Use proven LAPACK VARIANTS architecture for algorithm alternatives
**Implementation**: AlphaTensor follows existing lu/cholesky variant patterns
**Benefits**: Battle-tested integration, link-time selection, identical APIs

**VARIANTS Architecture**:
```
SRC/VARIANTS/
â”œâ”€â”€ alphatensor/                 # NEW: AlphaTensor implementation
â”‚   â”œâ”€â”€ dgemm_alpha.f           # Core AlphaTensor Fortran routine
â”‚   â”œâ”€â”€ test_dgemm_alpha.f      # Validation and testing
â”‚   â””â”€â”€ benchmark_dgemm_alpha.f # Performance benchmarking
â”œâ”€â”€ cholesky/                   # Existing: Cholesky factorization variants
â”‚   â”œâ”€â”€ RL/ â†’ cholrl.a         # Right-Looking variant library
â”‚   â””â”€â”€ TOP/ â†’ choltop.a       # Top-Level variant library
â””â”€â”€ lu/                         # Existing: LU factorization variants
    â”œâ”€â”€ CR/ â†’ lucr.a           # Crout variant library
    â”œâ”€â”€ LL/ â†’ lull.a           # Left-Looking variant library
    â””â”€â”€ REC/ â†’ lurec.a         # Recursive variant library
```

**Integration Strategy**:
```c
// AlphaTensor dispatch follows proven VARIANTS pattern
int DGEMM_ALPHA(char TRANSA, char TRANSB, int M, int N, int K,
                double ALPHA, double* A, int LDA, double* B, int LDB,
                double BETA, double* C, int LDC) {
    
    // 4x4 optimization detection (AlphaTensor's sweet spot)
    if (M == 4 && N == 4 && K == 4) {
        return dgemm_alphatensor_4x4(TRANSA, TRANSB, M, N, K, 
                                   ALPHA, A, LDA, B, LDB, BETA, C, LDC);
    }
    
    // Fallback to standard DGEMM for other sizes
    return DGEMM_(&TRANSA, &TRANSB, &M, &N, &K, &ALPHA, A, &LDA, 
                  B, &LDB, &BETA, C, &LDC);
}
```

**Build Integration** (follows existing VARIANTS pattern):
```makefile
# SRC/VARIANTS/Makefile (enhanced)
ALPHATENSOR = alphatensor/dgemm_alpha.o
alphatensor.a: $(ALPHATENSOR)
	$(ARCH) $(ARCHFLAGS) $@ $(ALPHATENSOR)
	$(RANLIB) $@
```

### Pattern 2: Container-First Development Architecture âœ…
**Principle**: All development, testing, and deployment through containers
**Implementation**:
- Multi-stage Docker builds for different environments
- GPU passthrough for development and production
- Volume mounts for live development workflow

**Container Hierarchy**:
```dockerfile
# Base Layer: System dependencies and compilers
FROM python:3.11-slim as lapack-base
RUN apt-get update && apt-get install -y \
    build-essential gfortran cmake \
    opencl-headers ocl-icd-opencl-dev

# Development Layer: Tools and development environment  
FROM lapack-base as lapack-dev
RUN pip install jupyterlab ipywidgets plotly
EXPOSE 8888 5000 8080

# Production Layer: Minimal runtime optimization
FROM python:3.11-slim as lapack-prod
COPY --from=builder /app /app
RUN apt-get install -y --no-install-recommends \
    libgfortran5 libopenblas0 ocl-icd-libopencl1
```

### Pattern 2: Non-Invasive Enhancement (Maintained) âœ…
**Principle**: Extend LAPACK without modifying existing Fortran source
**Implementation**:
- Wrapper functions around core LAPACK routines
- GPU dispatch layer sits above LAPACK, not within
- Preserve original function signatures for compatibility

**Enhanced Pattern with Containerization**:
```c
// GPU-enhanced wrapper in containerized environment
int DGESVDOCL_wrapper(char jobu, char jobvt, int m, int n, 
                      double* a, int lda, double* s, 
                      double* u, int ldu, double* vt, int ldvt,
                      double* work, int lwork, int* info) {
    
    // Container environment check
    if (container_has_gpu() && opencl_device_available() && 
        matrix_size_suitable(m, n)) {
        return DGESVDOCL_gpu(jobu, jobvt, m, n, a, lda, s, u, ldu, vt, ldvt, work, lwork, info);
    }
    
    // Fallback to original LAPACK
    return DGESVD_(&jobu, &jobvt, &m, &n, a, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, info);
}
```

### Pattern 3: Smart Dispatch Architecture (Enhanced) âœ…
**Principle**: Container-aware execution path optimization
**Enhanced Decision Tree**:
```
Input Matrix + Operation + Container Context
â”œâ”€â”€ Container GPU Available?
â”‚   â”œâ”€â”€ Yes: GPU Memory Sufficient?
â”‚   â”‚   â”œâ”€â”€ Yes: Matrix Size > GPU Threshold?
â”‚   â”‚   â”‚   â”œâ”€â”€ Yes: Execute GPU Kernel
â”‚   â”‚   â”‚   â””â”€â”€ No: Execute CPU (Small Matrix Optimization)
â”‚   â”‚   â””â”€â”€ No: Execute CPU with Memory Warning
â”‚   â””â”€â”€ No: Check Host GPU Passthrough
â”‚       â”œâ”€â”€ Available: Retry GPU Path
â”‚       â””â”€â”€ Unavailable: Execute CPU Fallback
```

**Container-Aware Implementation**:
```c
typedef enum {
    EXECUTION_CPU_FALLBACK,
    EXECUTION_GPU_OPENCL,
    EXECUTION_CPU_OPTIMIZED,
    EXECUTION_GPU_CONTAINER,
    EXECUTION_HOST_PASSTHROUGH
} execution_strategy_t;

execution_strategy_t choose_execution_strategy(int m, int n, operation_type_t op) {
    // Check container environment first
    if (is_running_in_container()) {
        if (!container_gpu_passthrough_available()) {
            log_debug("Container without GPU passthrough, using CPU");
            return EXECUTION_CPU_FALLBACK;
        }
    }
    
    if (!opencl_available()) {
        log_debug("OpenCL not available, using CPU fallback");
        return EXECUTION_CPU_FALLBACK;
    }
    
    size_t required_memory = calculate_gpu_memory_requirement(m, n, op);
    size_t available_memory = get_gpu_memory_available();
    
    if (required_memory > available_memory) {
        log_warning("Insufficient GPU memory (%zu MB required, %zu MB available)", 
                   required_memory / 1048576, available_memory / 1048576);
        return EXECUTION_CPU_FALLBACK;
    }
    
    if (matrix_size_benefits_from_gpu(m, n, op)) {
        return is_running_in_container() ? EXECUTION_GPU_CONTAINER : EXECUTION_GPU_OPENCL;
    }
    
    return EXECUTION_CPU_OPTIMIZED;
}
```

### Pattern 4: Docker Multi-Stage Build Optimization âœ…
**Principle**: Minimize production container size while maintaining development capabilities
**Implementation Strategy**:

```dockerfile
# Stage 1: Build Environment (Full Development Stack)
FROM python:3.11-slim as builder
RUN apt-get update && apt-get install -y \
    build-essential gfortran cmake make \
    opencl-headers ocl-icd-opencl-dev \
    libblas-dev liblapack-dev libopenblas-dev

# Install Python dependencies in virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Build LAPACK enhancements
COPY . /src
WORKDIR /src
RUN cmake -B build -DCMAKE_BUILD_TYPE=Release \
    -DWITH_GPU_SUPPORT=ON -DWITH_PYTHON_BINDINGS=ON
RUN cmake --build build -j$(nproc)

# Stage 2: Runtime Environment (Minimal Production)
FROM python:3.11-slim as runtime
RUN apt-get update && apt-get install -y --no-install-recommends \
    libgfortran5 libopenblas0 liblapack3 ocl-icd-libopencl1 \
    && rm -rf /var/lib/apt/lists/*

# Copy only production artifacts
COPY --from=builder /opt/venv /opt/venv
COPY --from=builder /src/build/lib* /usr/local/lib/
COPY --from=builder /src/build/python/ /opt/venv/lib/python3.11/site-packages/

# Create non-root user for security
RUN useradd --create-home --shell /bin/bash lapack
USER lapack
WORKDIR /home/lapack

ENV PATH="/opt/venv/bin:$PATH"
CMD ["python", "-c", "import lapack_py; print('LAPACK AI ready')"]
```

### Pattern 5: Zero-Copy NumPy Integration (Enhanced) âœ…
**Principle**: Container-optimized memory management for NumPy arrays
**Implementation**:
- Direct pointer access with container memory limits awareness
- Automatic memory layout detection and conversion
- Container resource monitoring integration

**Enhanced pybind11 Pattern**:
```cpp
py::array_t<double> svd_wrapper(py::array_t<double> input_matrix, 
                               bool use_gpu = true, 
                               bool monitor_performance = false) {
    
    // Container resource check
    if (monitor_performance) {
        container_metrics_start();
    }
    
    // Zero-copy access to NumPy data
    py::buffer_info buf_info = input_matrix.request();
    double* data_ptr = static_cast<double*>(buf_info.ptr);
    
    // Dimension extraction
    int m = buf_info.shape[0];
    int n = buf_info.shape[1];
    
    // Container memory limit check
    size_t matrix_size = m * n * sizeof(double);
    if (is_running_in_container() && matrix_size > get_container_memory_limit() * 0.8) {
        py::print(f"Warning: Matrix size {matrix_size/1048576:.1f}MB approaches container limit");
    }
    
    // Memory layout optimization
    bool is_c_contiguous = input_matrix.attr("flags").attr("c_contiguous").cast<bool>();
    if (!is_c_contiguous) {
        // Handle Fortran-contiguous or non-contiguous arrays
        input_matrix = py::array_t<double>::ensure(input_matrix, py::array::c_style);
        data_ptr = static_cast<double*>(input_matrix.mutable_data());
    }
    
    // Call optimized routine with container context
    auto result = call_lapack_svd_container_aware(data_ptr, m, n, use_gpu);
    
    if (monitor_performance) {
        auto metrics = container_metrics_stop();
        py::print(f"GPU utilization: {metrics.gpu_utilization}%, Memory: {metrics.memory_usage}MB");
    }
    
    return result;
}
```

### Pattern 6: Container Orchestration for Development âœ…
**Principle**: Streamlined development workflow through Docker Compose
**Architecture**:

```yaml
# docker-compose.dev.yml - Development Environment Orchestration
version: '3.8'

services:
  # Base development service with GPU support
  dev-base: &dev-base
    build:
      context: .
      dockerfile: infrastructure/Dockerfile.dev
    volumes:
      - .:/workspace
      - ~/.ssh:/home/lapack/.ssh:ro
      - build_cache:/workspace/build
      - opencl_cache:/home/lapack/.cache/opencl
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LAPACK_AI_DEBUG=1
      - LOG_LEVEL=DEBUG
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Interactive development shell
  shell:
    <<: *dev-base
    command: /bin/bash
    stdin_open: true
    tty: true

  # Jupyter notebook server
  jupyter:
    <<: *dev-base
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
    ports:
      - "8888:8888"

  # Flask development server
  flask:
    <<: *dev-base
    command: python -m flask run --host=0.0.0.0 --port=5000
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1

  # Testing service
  test:
    <<: *dev-base
    command: python -m pytest testing/ -v
    
  # Build service for testing builds
  build:
    <<: *dev-base
    command: >
      bash -c "
        cmake -B build -DCMAKE_BUILD_TYPE=Debug -DWITH_GPU_SUPPORT=ON &&
        cmake --build build -j$(nproc) &&
        ctest --test-dir build --output-on-failure
      "

volumes:
  build_cache:
  opencl_cache:
```

## GPU Integration Architecture (Enhanced)

### Container-Native OpenCL Setup âœ…
**Pattern**: GPU access through container runtime
**Implementation**:

```dockerfile
# GPU-enabled container configuration
FROM lapack-base as gpu-enabled

# Install NVIDIA Container Toolkit compatibility
RUN apt-get update && apt-get install -y \
    nvidia-container-toolkit-base \
    ocl-icd-libopencl1

# OpenCL environment configuration
ENV OCL_ENABLE_DEBUG=1
ENV PYOPENCL_COMPILER_OUTPUT=1
ENV OCL_CACHE_DIR=/home/lapack/.cache/opencl
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# GPU health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import pyopencl as cl; cl.get_platforms()" || exit 1
```

### OpenCL Kernel Organization (Container-Optimized) âœ…
**Structure**:
```
infrastructure/gpu_kernels/
â”œâ”€â”€ svd/
â”‚   â”œâ”€â”€ svd_reduction.cl      # Matrix reduction to bidiagonal form
â”‚   â”œâ”€â”€ svd_iteration.cl      # Iterative SVD computation
â”‚   â””â”€â”€ svd_postprocess.cl    # Back-transformation
â”œâ”€â”€ matrix_multiply/
â”‚   â”œâ”€â”€ gemm_batched.cl       # Batched matrix multiplication
â”‚   â”œâ”€â”€ gemm_tiled.cl         # Tiled memory optimization
â”‚   â”œâ”€â”€ gemm_alphatensor.cl   # AlphaTensor 4x4 optimization
â”‚   â””â”€â”€ gemm_mixed_precision.cl
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ matrix_utils.cl       # Common matrix operations
â”‚   â”œâ”€â”€ memory_transfer.cl    # Optimized host-device transfer
â”‚   â””â”€â”€ container_utils.cl    # Container-specific optimizations
â””â”€â”€ testing/
    â”œâ”€â”€ kernel_validation.cl  # Test kernels for CI/CD
    â””â”€â”€ performance_benchmark.cl
```

### Container-Aware Memory Management âœ…
**Principle**: Respect container resource limits
**Implementation**:
```c
typedef struct {
    cl_mem device_buffer;
    size_t allocated_size;
    size_t used_size;
    bool is_persistent;
    size_t container_memory_limit;
} gpu_memory_pool_t;

// Container-aware memory allocation
cl_mem allocate_gpu_memory_container(size_t size, bool persistent) {
    // Check container memory limits
    size_t container_limit = get_container_memory_limit();
    size_t current_usage = get_container_memory_usage();
    
    if (current_usage + size > container_limit * 0.9) {
        log_warning("GPU allocation would exceed container memory limit");
        return NULL;
    }
    
    // Check GPU memory availability
    size_t gpu_available = get_gpu_memory_available();
    if (size > gpu_available) {
        log_warning("Insufficient GPU memory (%zu MB requested, %zu MB available)", 
                   size / 1048576, gpu_available / 1048576);
        return NULL;
    }
    
    // Proceed with allocation
    return clCreateBuffer(context, CL_MEM_READ_WRITE, size, NULL, NULL);
}
```

## Performance Monitoring Architecture (Container-Enhanced)

### Metrics Collection System âœ…
**Pattern**: Container-aware performance data gathering
**Implementation**:
```python
class ContainerizedMetricsCollector:
    def __init__(self):
        self.metrics_queue = queue.Queue()
        self.gpu_monitor = GPUMonitor()
        self.cpu_monitor = CPUMonitor()
        self.container_monitor = ContainerMonitor()
        
    @contextmanager
    def measure_operation(self, operation_name, matrix_size):
        # Pre-operation metrics
        start_time = time.perf_counter()
        start_memory = self.cpu_monitor.get_memory_usage()
        start_container_memory = self.container_monitor.get_memory_usage()
        gpu_start = self.gpu_monitor.get_utilization()
        
        try:
            yield
        finally:
            # Post-operation metrics
            end_time = time.perf_counter()
            end_memory = self.cpu_monitor.get_memory_usage()
            end_container_memory = self.container_monitor.get_memory_usage()
            gpu_end = self.gpu_monitor.get_utilization()
            
            metrics = {
                'operation': operation_name,
                'matrix_size': matrix_size,
                'execution_time': end_time - start_time,
                'memory_delta': end_memory - start_memory,
                'container_memory_delta': end_container_memory - start_container_memory,
                'gpu_utilization': (gpu_start + gpu_end) / 2,
                'container_id': os.environ.get('HOSTNAME', 'unknown'),
                'gpu_passthrough': self.container_monitor.has_gpu_passthrough(),
                'timestamp': time.time()
            }
            self.metrics_queue.put(metrics)

class ContainerMonitor:
    def get_memory_usage(self):
        """Get container memory usage from cgroup."""
        try:
            with open('/sys/fs/cgroup/memory/memory.usage_in_bytes', 'r') as f:
                return int(f.read().strip())
        except FileNotFoundError:
            # Fallback for cgroup v2
            with open('/sys/fs/cgroup/memory.current', 'r') as f:
                return int(f.read().strip())
    
    def has_gpu_passthrough(self):
        """Check if container has GPU access."""
        return os.path.exists('/dev/nvidia0') or os.environ.get('NVIDIA_VISIBLE_DEVICES') == 'all'
```

### Dashboard Real-time Updates (Container-Optimized) âœ…
**Pattern**: Container service discovery and health monitoring
**Architecture**:
```python
# Flask-SocketIO with container awareness
@socketio.on('request_metrics')
def handle_metrics_request():
    container_info = {
        'container_id': os.environ.get('HOSTNAME', 'unknown'),
        'image': os.environ.get('IMAGE_NAME', 'lapack-ai-dev:latest'),
        'gpu_enabled': container_monitor.has_gpu_passthrough(),
        'memory_limit': container_monitor.get_memory_limit(),
        'cpu_limit': container_monitor.get_cpu_limit()
    }
    
    emit('container_info', container_info)
    
    while True:
        try:
            metrics = metrics_collector.get_latest_metrics()
            metrics['container_info'] = container_info
            emit('metrics_update', metrics)
            socketio.sleep(1)  # 1-second update interval
        except Exception as e:
            emit('error', {'message': str(e), 'container_id': container_info['container_id']})
```

## Testing and Validation Patterns (Container-Native)

### Container-Based Testing Framework âœ…
**Pattern**: All testing through containerized environments
```python
class ContainerizedTestRunner:
    def __init__(self):
        self.base_command = [
            'docker', 'run', '--rm', 
            '-v', f'{os.getcwd()}:/workspace',
            '--gpus', 'all'
        ]
    
    def run_test_suite(self, test_type='integration'):
        """Run tests in fresh container environment."""
        container_tag = 'lapack-ai-dev:latest'
        
        if test_type == 'unit':
            cmd = self.base_command + [
                container_tag, 'python', '-m', 'pytest', 
                'testing/unit/', '-v', '--tb=short'
            ]
        elif test_type == 'integration':
            cmd = self.base_command + [
                container_tag, 'python', 
                'testing/integration_tests.py'
            ]
        elif test_type == 'gpu':
            cmd = self.base_command + [
                '--entrypoint', 'python',
                container_tag, 'testing/gpu_validation.py'
            ]
        elif test_type == 'performance':
            cmd = self.base_command + [
                container_tag, 'python', 
                'testing/performance_benchmarks.py'
            ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        return {
            'exit_code': result.returncode,
            'stdout': result.stdout,
            'stderr': result.stderr,
            'container_tag': container_tag
        }

def test_numerical_accuracy_containerized(operation_func, reference_func, matrix_sizes, tolerance=1e-6):
    """Test numerical accuracy in isolated container."""
    for m, n in matrix_sizes:
        # Generate test matrix
        A = generate_test_matrix(m, n, condition_number=1e3)
        
        # Run in container
        test_runner = ContainerizedTestRunner()
        result = test_runner.run_container_test(
            'accuracy_test', 
            operation_name=operation_func.__name__,
            matrix_data=A.tolist(),
            tolerance=tolerance
        )
        
        assert result['accuracy_passed'], f"Container accuracy test failed: {result['error']}"
```

## Deployment and Packaging Patterns (Production-Ready)

### Kubernetes Deployment Pattern âœ…
**Pattern**: Production container orchestration
```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lapack-ai-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: lapack-ai
  template:
    metadata:
      labels:
        app: lapack-ai
    spec:
      containers:
      - name: lapack-ai
        image: lapack-ai-prod:latest
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
            nvidia.com/gpu: 1
          limits:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
        env:
        - name: LAPACK_GPU_ENABLED
          value: "true"
        - name: LAPACK_LOG_LEVEL
          value: "INFO"
        ports:
        - containerPort: 8080
        healthCheck:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
```

### Configuration Management Pattern (Container-Native) âœ…
**Pattern**: Environment-specific configuration without rebuilds
```python
class ContainerAwareLapackConfig:
    def __init__(self):
        # Container environment detection
        self.is_containerized = os.path.exists('/.dockerenv')
        self.container_id = os.environ.get('HOSTNAME', 'host')
        
        # GPU configuration
        self.gpu_enabled = self._detect_gpu_availability()
        self.gpu_memory_limit = int(os.getenv('LAPACK_GPU_MEMORY_LIMIT', '80'))
        
        # Container resource limits
        self.container_memory_limit = self._get_container_memory_limit()
        self.container_cpu_limit = self._get_container_cpu_limit()
        
        # Service configuration
        self.dashboard_port = int(os.getenv('LAPACK_DASHBOARD_PORT', '8080'))
        self.log_level = os.getenv('LAPACK_LOG_LEVEL', 'INFO')
        
        # Performance tuning
        self.batch_size_limit = min(
            int(os.getenv('LAPACK_BATCH_SIZE_LIMIT', '1024')),
            self._calculate_optimal_batch_size()
        )
    
    def _detect_gpu_availability(self):
        """Detect GPU availability in container."""
        if not self.is_containerized:
            return check_host_gpu_availability()
        
        # Check for NVIDIA container runtime
        nvidia_visible = os.environ.get('NVIDIA_VISIBLE_DEVICES')
        if nvidia_visible == 'all' or (nvidia_visible and nvidia_visible != 'none'):
            return True
        
        # Check for GPU device files
        return os.path.exists('/dev/nvidia0')
    
    def _get_container_memory_limit(self):
        """Get container memory limit from cgroups."""
        try:
            with open('/sys/fs/cgroup/memory/memory.limit_in_bytes', 'r') as f:
                limit = int(f.read().strip())
                # Handle unlimited case (very large number)
                return limit if limit < 2**63 - 1 else None
        except FileNotFoundError:
            return None
```

## Documentation Organization Pattern âœ…

### Subject-Based Documentation Architecture âœ…
**Pattern**: Logical separation of documentation by functional area
**Implementation**:
```
MODERNIZATION/
â”œâ”€â”€ analysis/                    # Strategic analysis and research
â”‚   â”œâ”€â”€ codebase_analysis.md           # LAPACK structure analysis
â”‚   â”œâ”€â”€ modernization_strategy.md     # Complete strategy document  
â”‚   â””â”€â”€ function_interface_mapping.md # API mapping
â”œâ”€â”€ implementation/              # Implementation guides and plans
â”‚   â”œâ”€â”€ phase1_implementation_plan.md # Phase 1 detailed plan
â”‚   â”œâ”€â”€ phase2_preparation_checklist.md # Phase 2 transition
â”‚   â””â”€â”€ docker_configuration.md       # Container setup guide
â”œâ”€â”€ testing/                     # All testing frameworks and scripts
â”‚   â”œâ”€â”€ environment_validation.py     # Phase 1 validation
â”‚   â”œâ”€â”€ integration_tests.py          # System integration tests
â”‚   â””â”€â”€ gpu_testing_setup.md          # GPU testing infrastructure
â”œâ”€â”€ infrastructure/              # Deployment and infrastructure
â”‚   â”œâ”€â”€ Dockerfile.base               # Foundation container
â”‚   â”œâ”€â”€ Dockerfile.dev                # Development container
â”‚   â”œâ”€â”€ Dockerfile.prod               # Production container
â”‚   â””â”€â”€ .dockerignore                 # Build optimization
â””â”€â”€ memory_bank/                 # AI memory system
    â”œâ”€â”€ memory_bank_projectbrief.md
    â”œâ”€â”€ mmemory_bank_productContext.md
    â”œâ”€â”€ mmemory_bank_systemPatterns.md (this file)
    â”œâ”€â”€ mmemory_bank_techContext.md
    â”œâ”€â”€ mmemory_bank_activeContext.md
    â””â”€â”€ mmemory_bank_progress.md
```

**Benefits**:
- ðŸŽ¯ **Clear Separation**: Each directory has focused responsibility
- ðŸŽ¯ **Intuitive Navigation**: Developers find documents by logical function
- ðŸŽ¯ **Scalable Growth**: Structure accommodates project expansion
- ðŸŽ¯ **Professional Organization**: Enterprise-grade documentation pattern

## Error Recovery and Resilience Patterns (Container-Enhanced)

### Container Health Monitoring Pattern âœ…
**Principle**: Proactive container health detection and recovery
```dockerfile
# Health check configuration in Dockerfile
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "
import sys
try:
    import lapack_py
    import pyopencl as cl
    # Test basic functionality
    lapack_py.test_basic_operations()
    print('Container healthy')
    sys.exit(0)
except Exception as e:
    print(f'Container unhealthy: {e}')
    sys.exit(1)
" || exit 1
```

### Graceful Degradation with Container Awareness âœ…
**Pattern**: Container resource-aware fallback strategies
```c
int robust_container_gpu_operation(operation_params_t* params) {
    // Check container resource availability
    if (!check_container_resources_sufficient(params)) {
        log_warning("Container resources insufficient, using optimized CPU fallback");
        return execute_cpu_optimized(params);
    }
    
    int result = attempt_gpu_execution(params);
    
    if (result == GPU_MEMORY_ERROR) {
        log_warning("GPU memory insufficient in container, falling back to CPU");
        // Update container memory tracking
        update_container_memory_usage(params);
        return execute_cpu_fallback(params);
    }
    
    if (result == GPU_DRIVER_ERROR) {
        log_error("GPU driver issue in container, disabling GPU for this session");
        disable_gpu_temporarily();
        return execute_cpu_fallback(params);
    }
    
    return result;
}
```

This enhanced system patterns document reflects our transition to a fully containerized development and deployment architecture, while maintaining the core LAPACK integration principles and adding robust GPU testing infrastructure capabilities. 